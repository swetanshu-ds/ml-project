{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "import os\n",
    "\n",
    "from src.utils import save_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    train_data_path:str = os.path.join(\"artifacts\",'train.csv')\n",
    "    test_data_pat:str = os.path.join('artifacts','test.csv')\n",
    "    raw_data_path:str = os.path.join('artifacts','data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_config = DataIngestionConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artifacts\\\\data.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestion_config.raw_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(ingestion_config.train_data_path),exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\test_project\\ml-project\\notebook\\data\\stud.csv')\n",
    "df.to_csv(ingestion_config.raw_data_path,index = False, header= True)\n",
    "#df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self):\n",
    "        self.ingestion_config = DataIngestionConfig()\n",
    "\n",
    "    def initiate_data_ingestion(self):\n",
    "        logging.info(\"Entered the data ingestion method or component\")\n",
    "        try:\n",
    "            df = pd.read_csv(r'D:\\test_project\\ml-project\\notebook\\data\\stud.csv')\n",
    "            logging.info(\"read the dataset as dataframe\")\n",
    "            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path),exist_ok=True)\n",
    "            df.to_csv(self.ingestion_config.raw_data_path,index = False, header= True)\n",
    "            logging.info(\"Train test split initiated\")\n",
    "            train_set,test_set =  train_test_split(df,test_size = 0.2,random_state = 42)\n",
    "            train_set.to_csv(self.ingestion_config.train_data_path,index = False,header = True)\n",
    "            test_set.to_csv(self.ingestion_config.test_data_path,index = False,header = True)\n",
    "            logging.info(\"Ingestion of the data is completed\") \n",
    "\n",
    "\n",
    "            return (\n",
    "                self.ingestion_config.train_data_path,\n",
    "                self.ingestion_config.test_data_path,\n",
    "                self.ingestion_config.raw_data_path,\n",
    "                \n",
    "            )\n",
    " \n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "import os\n",
    "\n",
    "from src.utils import save_object\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path=os.path.join('artifacts',\"proprocessor.pkl\")\n",
    "    \n",
    "    \n",
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config=DataTransformationConfig()\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        '''\n",
    "        This function si responsible for data trnasformation\n",
    "        \n",
    "        '''\n",
    "        try:\n",
    "            numerical_columns = [\"writing_score\", \"reading_score\"]\n",
    "            categorical_columns = [\n",
    "                \"gender\",\n",
    "                \"race_ethnicity\",\n",
    "                \"parental_level_of_education\",\n",
    "                \"lunch\",\n",
    "                \"test_preparation_course\",\n",
    "            ]\n",
    "\n",
    "            num_pipeline= Pipeline(\n",
    "                steps=[\n",
    "                (\"imputer\",SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\",StandardScaler())\n",
    "\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            cat_pipeline=Pipeline(\n",
    "\n",
    "                steps=[\n",
    "                (\"imputer\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"one_hot_encoder\",OneHotEncoder()),\n",
    "                (\"scaler\",StandardScaler(with_mean=False))\n",
    "                ]\n",
    "\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "            logging.info(f\"Numerical columns: {numerical_columns}\")\n",
    "\n",
    "            preprocessor=ColumnTransformer(\n",
    "                [\n",
    "                (\"num_pipeline\",num_pipeline,numerical_columns),\n",
    "                (\"cat_pipelines\",cat_pipeline,categorical_columns)\n",
    "\n",
    "                ]\n",
    "\n",
    "\n",
    "            )\n",
    "            print(\"#\"*100)\n",
    "            print(preprocessor)\n",
    "            return preprocessor\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
